# -*- coding: utf-8 -*-
"""multi-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DrEZ7-So9oOrS4N0OHeILJObp2CVy0EM
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import datetime
import os
import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
import pathlib
from google.colab import files

from zipfile import ZipFile
file_name = "/content/ds02-rs500-v02.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')
skip_training = False
batch_size = 32
img_height = 500
img_width = 500
IMG_SIZE = (500, 500)


train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  '/content/ds02-rs500-v02/train',
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  '/content/ds02-rs500-v02/validation',
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
num_classes = 5



#hyperparameters

train_frozen_epochs = 40
train_unfrozen_epochs = 30
train_learning_rate =0.0001
train_batch_size= 32
train_dropout=0.1
train_learning_rate_decrease = 100
unfrozen_layers = 100  #out of 175

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

#Base model
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
  tf.keras.layers.experimental.preprocessing.RandomContrast(
    factor=0.6),
])

preprocess_input = tf.keras.applications.resnet.preprocess_input

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()


prediction_layer = tf.keras.layers.Dense(5,activation='sigmoid')


inputs = tf.keras.Input(shape=(500, 500, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
#x= tf.keras.layers.BatchNormalization(axis=1)(x)
x = tf.keras.layers.Dropout(train_dropout)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

base_model.trainable = False

model.summary()

base_learning_rate = train_learning_rate



#Multiple classes
model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

print(base_learning_rate)
print(train_learning_rate_decrease)
print("Dropout:"+ str(train_dropout))
print("Number of layers in the base model: ", len(base_model.layers))

if skip_training==False:
  epochs=train_frozen_epochs 
  history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  batch_size=train_batch_size
    )
  base_model.trainable = True
  

  # Fine-tune from this layer onwards
  print("Number of layers in the base model: ", len(base_model.layers))

  fine_tune_at = unfrozen_layers

  # Freeze all the layers before the `fine_tune_at` layer
  for layer in base_model.layers[:fine_tune_at]:
    layer.trainable =  False

  fine_tune_epochs = train_unfrozen_epochs
  total_epochs =  epochs + fine_tune_epochs


  #Multiple classes
  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/train_learning_rate_decrease),
              metrics=['accuracy'])
  
  history_fine = model.fit(train_ds,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=val_ds,
                         batch_size = train_batch_size)




  model.save('model-test4')


else:
  pass
    model = tf.keras.models.load_model('model-test4')

print("Validation")
results = model.evaluate(val_ds)
print("val loss, vall acc:", results)

print(epochs)
if skip_training==False:
    acc = history.history['accuracy']
    print(len(acc))
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(epochs)
    acc += history_fine.history['accuracy']
    val_acc += history_fine.history['val_accuracy']

    loss += history_fine.history['loss']
    val_loss += history_fine.history['val_loss']

print(len(acc))

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0, 1.0])
plt.plot([epochs-1,epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([epochs-1,epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

from pandas import *

#Retrieve a batch of images from the test set
tt=val_ds.as_numpy_iterator()
image_batch, label_batch = tt.next()

#plt.figure(figsize=(10, 10))
#for i in range(15):
 # ax = plt.subplot(4, 4, i + 1)
  #plt.imshow(image_batch[i].astype("uint8"))
  #plt.title(class_names[predictions[i]] + class_names[label_batch[i]])
  #plt.axis("off")
#plt.show()

TP=0
FP=0
TN=0
FN=0
cont = 0
matrix = [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]
t = 229 // 32 

images = []
true=[]
predicted=[]
for i in range(t):
  predictions = model.predict_on_batch(image_batch)
  predictions= np.argmax(predictions, axis = 1)
  for j in range(len(predictions)): # 0 1 2 3 4 
    cont = cont + 1
    p = predictions[j]
    t = label_batch[j]
    if p != t:
      true.append(t)
      predicted.append(p)
      images.append(image_batch[j])
    matrix[t][p]+=1
  image_batch, label_batch = tt.next()


predictions = model.predict_on_batch(image_batch)
predictions= np.argmax(predictions, axis = 1)

for j in range(len(predictions)):
    cont = cont + 1
    p = predictions[j]
    t = label_batch[j]
    if p != t:
      images.append(image_batch[j])
      true.append(t)
      predicted.append(p)
    matrix[t][p]+=1




confusion_matrix = [[" ", class_names[0],class_names[1],class_names[2],class_names[3],class_names[4]],[class_names[0],matrix[0][0],matrix[0][1],matrix[0][2],matrix[0][3],matrix[0][4]],[class_names[1],matrix[1][0],matrix[1][1],matrix[1][2],matrix[1][3],matrix[1][4]],[class_names[2],matrix[2][0],matrix[2][1],matrix[2][2],matrix[2][3],matrix[2][4]],[class_names[3],matrix[3][0],matrix[3][1],matrix[3][2],matrix[3][3],matrix[3][4]],[class_names[4],matrix[4][0],matrix[4][1],matrix[4][2],matrix[4][3],matrix[4][4]]]
D = DataFrame(confusion_matrix)
print(D.to_string(index=False))

plt.figure(figsize=(10, 10))
for i in range(11):
  ax = plt.subplot(4, 4, i + 1)
  plt.imshow(images[i].astype("uint8"))
  plt.title(class_names[true[i]]+"/" + class_names[predicted[i]])
  plt.axis("off")
plt.show()
print('TOTAL VALIDATION IMAGES!!:'+str(cont))

#Precision and recall
for i in range(14):
  predictions = model.predict_on_batch(image_batch).flatten()
  predictions = tf.nn.sigmoid(predictions)
  predictions = tf.where(predictions < 0.5, 0, 1)
  for j in range(len(predictions)):
    cont = cont + 1
    p = predictions[j]
    t = label_batch[j]
    if (p==1) and (t==1):
      TP=TP+1
    if (p==1) and (t==0):
      FP=FP+1
    if (p==0) and (t==0):
      TN=TN+1
    if (p==0) and (t==1):
      FN=FN+1
  #next
  if i!=14:
    image_batch, label_batch = tt.next()