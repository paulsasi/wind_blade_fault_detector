# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xk-yAGIrn-NEKKX8-5qniDnMY_XtugG4
"""

import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import datetime
import os
import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
import pathlib
from google.colab import files

from zipfile import ZipFile
file_name = "/content/fold1.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print('Done')
skip_training = False
batch_size = 15
img_height = 224
img_width = 224
IMG_SIZE = (224, 224)


train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  '/content/fold1/train',
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  '/content/fold1/validation',
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
num_classes = 2

#hyperparameters

train_frozen_epochs = 50
train_unfrozen_epochs = 50
train_learning_rate =0.00001
train_batch_size= 32
train_dropout=0.5
train_learning_rate_decrease = 10
unfrozen_layers = 50  #out of 175

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

#Base model
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),
  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),
  tf.keras.layers.experimental.preprocessing.RandomContrast(
    factor=0.6),
])

preprocess_input = tf.keras.applications.resnet.preprocess_input

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()


prediction_layer = tf.keras.layers.Dense(1)


inputs = tf.keras.Input(shape=(224, 224, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
#x= tf.keras.layers.BatchNormalization(axis=1)(x)
x = tf.keras.layers.Dropout(train_dropout)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

base_model.trainable = False

model.summary()

base_learning_rate = train_learning_rate

#2 classes
model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])

#Multiple classes
#model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
 #             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
  #            metrics=['accuracy'])

print(base_learning_rate)
print(train_learning_rate_decrease)
print("Dropout:"+ str(train_dropout))
print("Number of layers in the base model: ", len(base_model.layers))

if skip_training==False:
  epochs=train_frozen_epochs 
  history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
  batch_size=train_batch_size
    )
  base_model.trainable = True
  

  # Fine-tune from this layer onwards
  print("Number of layers in the base model: ", len(base_model.layers))

  fine_tune_at = unfrozen_layers

  # Freeze all the layers before the `fine_tune_at` layer
  for layer in base_model.layers[:fine_tune_at]:
    layer.trainable =  False

  fine_tune_epochs = train_unfrozen_epochs
  total_epochs =  epochs + fine_tune_epochs

  #2 classes
  model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])
  #Multiple classes
  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer = tf.keras.optimizers.Adam(lr=base_learning_rate/train_learning_rate_decrease),
              metrics=['accuracy'])
  
  history_fine = model.fit(train_ds,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=val_ds,
                         batch_size = train_batch_size)




  model.save('model-test1')


else:
  pass
   # model = tf.keras.models.load_model('model-test01')

print("Validation")
results = model.evaluate(val_ds)
print("val loss, vall acc:", results)

print(epochs)
if skip_training==False:
    acc = history.history['accuracy']
    print(len(acc))
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(epochs)
    acc += history_fine.history['accuracy']
    val_acc += history_fine.history['val_accuracy']

    loss += history_fine.history['loss']
    val_loss += history_fine.history['val_loss']

print(len(acc))

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0.8, 1.0])
plt.plot([epochs-1,epochs-1],
          plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 1.0])
plt.plot([epochs-1,epochs-1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

#Retrieve a batch of images from the test set
tt=val_ds.as_numpy_iterator()
image_batch, label_batch = tt.next()

predictions = model.predict_on_batch(image_batch).flatten()

# Apply a sigmoid since our model returns logits
predictions = tf.nn.sigmoid(predictions)
predictions = tf.where(predictions < 0.5, 0, 1)

print('Predictions:\n', predictions.numpy())
print('Labels:\n', label_batch)





#plt.figure(figsize=(10, 10))
#for i in range(15):
 # ax = plt.subplot(4, 4, i + 1)
  #plt.imshow(image_batch[i].astype("uint8"))
  #plt.title(class_names[predictions[i]] + class_names[label_batch[i]])
  #plt.axis("off")
#plt.show()

TP=0
FP=0
TN=0
FN=0


cont = 0
#Precision and recall
for i in range(14):
  predictions = model.predict_on_batch(image_batch).flatten()
  predictions = tf.nn.sigmoid(predictions)
  predictions = tf.where(predictions < 0.5, 0, 1)
  for j in range(len(predictions)):
    cont = cont + 1
    p = predictions[j]
    t = label_batch[j]
    if (p==1) and (t==1):
      TP=TP+1
    if (p==1) and (t==0):
      FP=FP+1
    if (p==0) and (t==0):
      TN=TN+1
    if (p==0) and (t==1):
      FN=FN+1
  #next
  if i!=14:
    image_batch, label_batch = tt.next()

precision= TP / (TP+FP)
recall= TP / (TP+FN)
print('cont:'+str(cont))
print('TP:'+str(TP))
print('TN:'+str(TN))
print('FP:'+str(FP))
print('FN:'+str(FN))

print('Precision:'+str(precision))
print('Recall:'+str(recall))